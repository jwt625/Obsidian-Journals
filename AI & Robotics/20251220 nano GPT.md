
2025-12-20T21:53:17-08:00
Tried running a smaller model with 16 layers and 2 H100.

```python
#!/bin/bash

  

# Modified version of run1000.sh for 2×H100 80GB GPUs

# Adapted from the original 8×H100 configuration

# Model size reduced to fit available compute budget

  

# Parse command line arguments

SKIP_BASE_TRAINING=0

while [[ $# -gt 0 ]]; do

case $1 in

--skip-base-training)

SKIP_BASE_TRAINING=1

shift

;;

*)

echo "Unknown option: $1"

echo "Usage: $0 [--skip-base-training]"

exit 1

;;

esac

done

  

# all the setup stuff

export OMP_NUM_THREADS=1

export NANOCHAT_BASE_DIR="$HOME/.cache/nanochat"

mkdir -p $NANOCHAT_BASE_DIR

command -v uv &> /dev/null || curl -LsSf https://astral.sh/uv/install.sh | sh

[ -d ".venv" ] || uv venv

uv sync --extra gpu

source .venv/bin/activate

if [ -z "$WANDB_RUN" ]; then

WANDB_RUN=dummy

fi

python -m nanochat.report reset

curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y

source "$HOME/.cargo/env"

uv run maturin develop --release --manifest-path rustbpe/Cargo.toml

  

# Download identity_conversations.jsonl only if it doesn't exist or is corrupted (278 bytes = AWS 403 error)

IDENTITY_FILE="$NANOCHAT_BASE_DIR/identity_conversations.jsonl"

if [ ! -f "$IDENTITY_FILE" ] || [ $(stat -c%s "$IDENTITY_FILE") -eq 278 ]; then

echo "Downloading identity_conversations.jsonl from GitHub workaround (AWS S3 returns 403)..."

curl -L -o "$NANOCHAT_BASE_DIR/identity_conversations.jsonl.zip" "https://github.com/user-attachments/files/24214240/identity_conversations.jsonl.zip"

unzip -o "$NANOCHAT_BASE_DIR/identity_conversations.jsonl.zip" -d "$NANOCHAT_BASE_DIR"

rm -f "$NANOCHAT_BASE_DIR/identity_conversations.jsonl.zip"

echo "Downloaded identity_conversations.jsonl ($(stat -c%s "$IDENTITY_FILE") bytes)"

else

echo "identity_conversations.jsonl already exists ($(stat -c%s "$IDENTITY_FILE") bytes), skipping download"

fi

  

# train tokenizer on ~4B characters and kick off download of the rest for pretraining

if [ $SKIP_BASE_TRAINING -eq 0 ]; then

python -m nanochat.dataset -n 16

# start downloading the rest of the shards for a total of 200 (reduced from 800 for smaller model)

python -m nanochat.dataset -n 200 &

# todo: download the rest of it

python -m scripts.tok_train --max_chars=4000000000

python -m scripts.tok_eval

else

echo "Skipping base training setup (tokenizer and dataset download)"

fi

  

# Hyperparameter selection for 2×H100 configuration:

# Hardware: 2×H100 80GB (vs. original 8×H100)

# This gives us 1/4 the total compute capacity of the original setup.

#

# Model size selection:

# - Using depth=16 (vs. original depth=32)

# - Keeping device_batch_size=8 (should fit comfortably in 80GB VRAM)

# - Expected model size: ~470M parameters (vs. 1.9B in original)

# - Expected training tokens: ~9.4B (Chinchilla scaling: 20 × params)

# - Expected dataset requirement: ~45B chars / 250M chars per shard ≈ 180-200 shards

#

# The training script will automatically:

# - Calculate optimal number of training steps based on Chinchilla scaling

# - Adjust gradient accumulation to maintain effective batch size

# - Set learning rates appropriately

#

# Note: Training will take longer per step with fewer GPUs, but total steps will be

# fewer due to smaller model size. Monitor the first few steps to verify:

# - VRAM usage is reasonable (should be well under 80GB)

# - MFU is acceptable (target >40%)

# - Adjust depth or device_batch_size if needed

  

# Number of processes/GPUs to use

NPROC_PER_NODE=2

  

if [ $SKIP_BASE_TRAINING -eq 0 ]; then

echo "Running base training..."

torchrun --standalone --nproc_per_node=$NPROC_PER_NODE -m scripts.base_train -- --depth=16 --device_batch_size=8 --run=$WANDB_RUN

torchrun --standalone --nproc_per_node=$NPROC_PER_NODE -m scripts.base_loss

torchrun --standalone --nproc_per_node=$NPROC_PER_NODE -m scripts.base_eval

else

echo "Skipping base training (using existing checkpoint)"

fi

  

# midtrain

# NOTE: ensure that we use the same device_batch_size here as the base training script.

torchrun --standalone --nproc_per_node=$NPROC_PER_NODE -m scripts.mid_train -- --device_batch_size=8 --run=$WANDB_RUN

torchrun --standalone --nproc_per_node=$NPROC_PER_NODE -m scripts.chat_eval -- -i mid

  

# sft

torchrun --standalone --nproc_per_node=$NPROC_PER_NODE -m scripts.chat_sft -- --run=$WANDB_RUN

torchrun --standalone --nproc_per_node=$NPROC_PER_NODE -m scripts.chat_eval -- -i sft

  

# generate final report

python -m nanochat.report generate

  

# talk to it

python -m scripts.chat_web
```


![[Pasted image 20251220215332.png]]
- look at this sweet utilization dip.


2025-12-21T10:32:14-08:00

had to bugfix a few data files.
- `identity_conversations.jsonl.zip`


Got the power and temperature metrics, as well as the final chat running:
![[Pasted image 20251221103300.png]]
- `gpu_metrics_20251221_070042.csv`
- `nanochat/training_20251221_070319.log`

Subsequent training steps and logs that did not work until the missing `jsonl` file bugfix:
- `nanochat/training_20251221_172457.log`

![[Pasted image 20251221103241.png]]

# Depth 24
2025-12-21T10:42:13-08:00
Start training a depth 24 model instead of 16.



